Day 2:


- What is sent across in channel: RModel(Residual), PModel(Predictive), Quantization Factor 

- Entropy coding is a lossless process, 


Self-information = (1/Log P(s)) = -log P(s) -> a value where lim x->1 means (l/Log P(s)) -> 0

entropy = weighted average of all possible values


ex:

S   | P(S)


A 	.5

B	.25

C 	.125

D	.125



= -1/2 log(.5) + -1/4 log(.25) - 1/8 log(0.125) - 1/8 log(0.125)

= 1 3/8 bits/symbol average

Huffman Encoding -> assigning 1s and 0s to represent symbols being passed through a channel
- read an example about this later
- only works if probablities are powers of 2



Arthmetic Encoding -> 


	
	|------
	|
	|
	|
	|	
	|
	|-----

